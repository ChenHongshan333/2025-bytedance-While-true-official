{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3cc4b537",
      "metadata": {},
      "source": [
        "# Fine-tuning BERT on Review Classification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "evaluation",
      "metadata": {},
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluate_model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "if trainer is not None:\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation Results:\")\n",
        "    for key, value in eval_results.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "else:\n",
        "    print(\"Cannot evaluate - no trained model available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "detailed_evaluation",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed evaluation with classification report\n",
        "if trainer is not None:\n",
        "    # Get predictions on validation set\n",
        "    predictions = trainer.predict(tokenized_dataset['validation'])\n",
        "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "    y_true = predictions.label_ids\n",
        "    \n",
        "    # Print detailed classification report\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true, y_pred, \n",
        "        target_names=CATEGORIES,\n",
        "        digits=4\n",
        "    ))\n",
        "    \n",
        "    # Confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm_df = pd.DataFrame(cm, index=CATEGORIES, columns=CATEGORIES)\n",
        "    display(cm_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inference",
      "metadata": {},
      "source": [
        "## Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inference_function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_reviews(model, tokenizer, texts, batch_size=16):\n",
        "    \"\"\"Make predictions on a list of review texts.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    \n",
        "    # Ensure model is on the right device\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        \n",
        "        # Tokenize batch\n",
        "        inputs = tokenizer(\n",
        "            batch, \n",
        "            padding=True, \n",
        "            truncation=True, \n",
        "            max_length=512, \n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Move inputs to same device as model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            preds = torch.argmax(probs, dim=-1)\n",
        "            \n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_inference",
      "metadata": {},
      "outputs": [],
      "source": [
        "if trainer is not None:\n",
        "    # Extract the base model for inference (important for multi-GPU)\n",
        "    if hasattr(trainer.model, 'module'):\n",
        "        # Model is wrapped in DataParallel/DistributedDataParallel\n",
        "        inference_model = trainer.model.module\n",
        "    else:\n",
        "        # Single GPU or CPU\n",
        "        inference_model = trainer.model\n",
        "    \n",
        "    # Move to single GPU for inference\n",
        "    inference_model = inference_model.to('cuda:0')\n",
        "    inference_model.eval()\n",
        "    \n",
        "    # Sample some validation examples\n",
        "    sample_indices = random.sample(range(len(tokenized_dataset['validation'])), 5)\n",
        "    sample_texts = [dataset['validation'][i]['combined_text'] for i in sample_indices]\n",
        "    sample_labels = [tokenized_dataset['validation'][i]['labels'] for i in sample_indices]\n",
        "    \n",
        "    # Make predictions with the unwrapped model\n",
        "    predictions = predict_reviews(inference_model, tokenizer, sample_texts)\n",
        "    \n",
        "    print(\"Sample Predictions:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i, (text, true_label, pred_label) in enumerate(zip(sample_texts, sample_labels, predictions)):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Text: {text[:150]}...\" if len(text) > 150 else f\"Text: {text}\")\n",
        "        print(f\"True Label: {CATEGORIES[true_label]}\")\n",
        "        print(f\"Predicted: {CATEGORIES[pred_label]}\")\n",
        "        print(f\"Correct: {'✓' if true_label == pred_label else '✗'}\")\n",
        "        print(\"-\" * 40)\n",
        "else:\n",
        "    print(\"Cannot test inference - no trained model available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2be6ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def save_evaluation_results(trainer, tokenized_dataset, categories, output_dir=\"./evaluation_results\"):\n",
        "    \"\"\"\n",
        "    Save comprehensive evaluation results to CSV files\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    if trainer is not None:\n",
        "        # Get predictions on validation set\n",
        "        predictions = trainer.predict(tokenized_dataset['validation'])\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = predictions.label_ids\n",
        "        \n",
        "        # 1. Per-sample results CSV\n",
        "        validation_results = []\n",
        "        for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n",
        "            # Get the original text\n",
        "            sample_text = tokenized_dataset['validation'][i]['input_ids']\n",
        "            decoded_text = trainer.tokenizer.decode(sample_text, skip_special_tokens=True)\n",
        "            \n",
        "            # Get prediction confidence\n",
        "            probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions[i]), dim=0)\n",
        "            confidence = float(probs[pred_label])\n",
        "            \n",
        "            validation_results.append({\n",
        "                'sample_id': i,\n",
        "                'text_preview': decoded_text[:100] + \"...\" if len(decoded_text) > 100 else decoded_text,\n",
        "                'true_label': categories[true_label],\n",
        "                'predicted_label': categories[pred_label],\n",
        "                'correct': true_label == pred_label,\n",
        "                'confidence': confidence,\n",
        "                'true_label_id': int(true_label),\n",
        "                'predicted_label_id': int(pred_label)\n",
        "            })\n",
        "        \n",
        "        # Save per-sample results\n",
        "        results_df = pd.DataFrame(validation_results)\n",
        "        results_df.to_csv(f\"{output_dir}/validation_predictions.csv\", index=False)\n",
        "        print(f\"Per-sample results saved to: {output_dir}/validation_predictions.csv\")\n",
        "        \n",
        "        # 2. Classification metrics CSV\n",
        "        report_dict = classification_report(y_true, y_pred, target_names=categories, output_dict=True)\n",
        "        \n",
        "        # Convert to DataFrame format\n",
        "        metrics_data = []\n",
        "        for label in categories:\n",
        "            metrics_data.append({\n",
        "                'category': label,\n",
        "                'precision': report_dict[label]['precision'],\n",
        "                'recall': report_dict[label]['recall'],\n",
        "                'f1_score': report_dict[label]['f1-score'],\n",
        "                'support': int(report_dict[label]['support'])\n",
        "            })\n",
        "        \n",
        "        # Add overall metrics\n",
        "        metrics_data.append({\n",
        "            'category': 'macro_avg',\n",
        "            'precision': report_dict['macro avg']['precision'],\n",
        "            'recall': report_dict['macro avg']['recall'],\n",
        "            'f1_score': report_dict['macro avg']['f1-score'],\n",
        "            'support': int(report_dict['macro avg']['support'])\n",
        "        })\n",
        "        \n",
        "        metrics_data.append({\n",
        "            'category': 'weighted_avg',\n",
        "            'precision': report_dict['weighted avg']['precision'],\n",
        "            'recall': report_dict['weighted avg']['recall'],\n",
        "            'f1_score': report_dict['weighted avg']['f1-score'],\n",
        "            'support': int(report_dict['weighted avg']['support'])\n",
        "        })\n",
        "        \n",
        "        metrics_df = pd.DataFrame(metrics_data)\n",
        "        metrics_df.to_csv(f\"{output_dir}/classification_metrics.csv\", index=False)\n",
        "        print(f\"Classification metrics saved to: {output_dir}/classification_metrics.csv\")\n",
        "        \n",
        "        # 3. Confusion matrix CSV\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        cm_df = pd.DataFrame(cm, index=categories, columns=categories)\n",
        "        cm_df.to_csv(f\"{output_dir}/confusion_matrix.csv\")\n",
        "        print(f\"Confusion matrix saved to: {output_dir}/confusion_matrix.csv\")\n",
        "        \n",
        "        # 4. Summary statistics CSV\n",
        "        total_samples = len(y_true)\n",
        "        correct_predictions = np.sum(y_true == y_pred)\n",
        "        accuracy = correct_predictions / total_samples\n",
        "        \n",
        "        summary_data = [{\n",
        "            'metric': 'total_samples',\n",
        "            'value': total_samples\n",
        "        }, {\n",
        "            'metric': 'correct_predictions', \n",
        "            'value': correct_predictions\n",
        "        }, {\n",
        "            'metric': 'accuracy',\n",
        "            'value': accuracy\n",
        "        }, {\n",
        "            'metric': 'training_samples',\n",
        "            'value': len(tokenized_dataset['train'])\n",
        "        }]\n",
        "        \n",
        "        # Add per-class accuracy\n",
        "        for i, category in enumerate(categories):\n",
        "            class_mask = (y_true == i)\n",
        "            if np.sum(class_mask) > 0:\n",
        "                class_accuracy = np.sum((y_true == y_pred) & class_mask) / np.sum(class_mask)\n",
        "                summary_data.append({\n",
        "                    'metric': f'{category}_accuracy',\n",
        "                    'value': class_accuracy\n",
        "                })\n",
        "        \n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_df.to_csv(f\"{output_dir}/evaluation_summary.csv\", index=False)\n",
        "        print(f\"Evaluation summary saved to: {output_dir}/evaluation_summary.csv\")\n",
        "        \n",
        "        # 5. Training history (if available)\n",
        "        if hasattr(trainer.state, 'log_history'):\n",
        "            history_df = pd.DataFrame(trainer.state.log_history)\n",
        "            history_df.to_csv(f\"{output_dir}/training_history.csv\", index=False)\n",
        "            print(f\"Training history saved to: {output_dir}/training_history.csv\")\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'total_samples': total_samples,\n",
        "            'results_files': [\n",
        "                f\"{output_dir}/validation_predictions.csv\",\n",
        "                f\"{output_dir}/classification_metrics.csv\", \n",
        "                f\"{output_dir}/confusion_matrix.csv\",\n",
        "                f\"{output_dir}/evaluation_summary.csv\"\n",
        "            ]\n",
        "        }\n",
        "    else:\n",
        "        print(\"Cannot save evaluation results - no trained model available\")\n",
        "        return None\n",
        "\n",
        "# Usage after training:\n",
        "if trainer is not None:\n",
        "    eval_results = save_evaluation_results(trainer, tokenized_dataset, CATEGORIES)\n",
        "    print(f\"\\nEvaluation complete! Overall accuracy: {eval_results['accuracy']:.4f}\")\n",
        "else:\n",
        "    print(\"No trainer available for evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f3dce7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate evaluation results as CSV with validation predictions\n",
        "if trainer is not None:\n",
        "    print(\"Generating evaluation CSV with validation predictions...\")\n",
        "    \n",
        "    # Get predictions on validation set\n",
        "    predictions = trainer.predict(tokenized_dataset['validation'])\n",
        "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "    y_true = predictions.label_ids\n",
        "    probabilities = torch.nn.functional.softmax(torch.from_numpy(predictions.predictions), dim=-1)\n",
        "    \n",
        "    # Get the original validation data\n",
        "    val_indices = val_df.index.tolist()  # Get original indices\n",
        "    \n",
        "    # Create evaluation results dataframe\n",
        "    eval_results = pd.DataFrame({\n",
        "        'original_index': val_indices,\n",
        "        'true_category': [CATEGORIES[label] for label in y_true],\n",
        "        'predicted_category': [CATEGORIES[pred] for pred in y_pred],\n",
        "        'classification_result': ['valid' if CATEGORIES[pred] == 'valid' else 'invalid' for pred in y_pred],\n",
        "        'valid_probability': probabilities[:, 0].numpy(),  # Probability of 'valid' (index 0)\n",
        "        'correct_prediction': y_true == y_pred\n",
        "    })\n",
        "    \n",
        "    # Add the original text for reference\n",
        "    eval_results['review_text'] = [val_dataset[i]['combined_text'] for i in range(len(val_dataset))]\n",
        "    \n",
        "    # Save to CSV\n",
        "    eval_csv_path = \"validation_evaluation_results.csv\"\n",
        "    eval_results.to_csv(eval_csv_path, index=False)\n",
        "    \n",
        "    print(f\"Evaluation results saved to: {eval_csv_path}\")\n",
        "    print(f\"Total validation samples: {len(eval_results)}\")\n",
        "    print(f\"Correct predictions: {eval_results['correct_prediction'].sum()}\")\n",
        "    print(f\"Accuracy: {eval_results['correct_prediction'].mean():.4f}\")\n",
        "    \n",
        "    # Show sample of results\n",
        "    print(f\"\\nSample of evaluation results:\")\n",
        "    display(eval_results.head(10))\n",
        "    \n",
        "else:\n",
        "    print(\"No trained model available for evaluation CSV generation\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cftenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
